histfun18 <-
function(i) {
hist(
C18[, i],
ylim = c(0, 1500),
col = rgb(0, 0.2, 0, 0.5),
main = NULL,
xlab = print(paste(names(C[i])))
)
hist(
I18[, i],
ylim = c(0, 1500),
col = rgb(0.6, 1, 0, 0.25),
add = T
)
}
lapply(c(1:5), histfun18)
# Prepare the data for machine learning models
dataPV15 <-
dplyr::select(dataB15, c("GEND", starts_with("PV")))
dataD15 <- cbind(dataI15, dataPV15)
View(I15)
View(C18)
dataD15 <- cbind(dataI15, dataPV15)
View(dataI15)
# gender as factor recoded as 0 and 1
dataD15$GEND <- as.factor(car::recode(dataD15$GEND, "'2' = 1; '1' = 0"))
# gender as factor recoded as 0 and 1
dataD15$GEND <- as.factor(car::recode(dataD15$GEND, "'2' = 1; '1' = 0"))
# Prepare the data for machine learning models
dataPV15 <-
dplyr::select(dataB15, c("GEND", starts_with("PV")))
dataD15 <- cbind(dataI15, dataPV15)
# gender as factor recoded as 0 and 1
dataD15$GEND <- as.factor(car::recode(dataD15$GEND, "'2' = 1; '1' = 0"))
# levels of proficiency
dataD15$meansM <-
rowMeans(subset(dataD15, select = c(PV1MATH:PV10MATH)))
View(dataD15)
dataD15$meansS <-
rowMeans(subset(dataD15, select = c(PV1SCIE:PV10SCIE)))
dataD15$Sc <-
cut(dataD15$meansS,
c(0, 409.54, 633.33, 1000),
labels = c("1", "2", "3"))
dataSc15 <-
dplyr::select(dataD15, c("INTE", "COMP", "AUTO", "SOCI", "ESCS", "GEND", "Sc"))
dataD15$M <-
cut(dataD15$meansM,
c(0, 420.07, 606.99, 1000),
labels = c("1", "2", "3"))
dataM15 <-
dplyr::select(dataD15, c("INTE", "COMP", "AUTO", "SOCI", "ESCS", "GEND", "M"))
# the same for 2018
dataPV18 <-
dplyr::select(dataB18, c("GEND", starts_with("PV")))
dataD18 <- cbind(dataI18, dataPV18)
# gender as factor recoded as 0 and 1
dataD18$GEND <- as.factor(car::recode(dataD18$GEND, "'2' = 1; '1' = 0"))
# levels of proficiency
dataD18$meansM <-
rowMeans(subset(dataD18, select = c(PV1MATH:PV10MATH)))
dataD18$meansS <-
rowMeans(subset(dataD18, select = c(PV1SCIE:PV10SCIE)))
dataD18$Sc <-
cut(dataD18$meansS,
c(0, 409.54, 633.33, 1000),
labels = c("1", "2", "3"))
dataSc18 <-
dplyr::select(dataD18, c("INTE", "COMP", "AUTO", "SOCI", "ESCS", "GEND", "Sc"))
dataD18$M <-
cut(dataD18$meansM,
c(0, 420.07, 606.99, 1000),
labels = c("1", "2", "3"))
dataM18 <-
dplyr::select(dataD18, c("INTE", "COMP", "AUTO", "SOCI", "ESCS", "GEND", "M"))
# Look at imbalanced classes
for (i in 1:3) {
print(paste(nrow(dataM15[dataM15$M == i, ])))
}
for (i in 1:3) {
print(paste(nrow(dataSc15[dataSc15$Sc == i, ])))
}
for (i in 1:3) {
print(paste(nrow(dataM18[dataM18$M == i, ])))
}
for (i in 1:3) {
print(paste(nrow(dataSc18[dataSc18$Sc == i, ])))
}
# Split 2015 into training set and test set
set.seed(100)
indSc15 <-
sample(2, nrow(dataSc15), replace = TRUE, prob = c(.8, .2))
indSc15
nrow(dataSc15)
table(indSc15)
trainMU15
indM15 <- sample(2, nrow(dataM15), replace = TRUE, prob = c(.8, .2))
trainMU15 <- dataM15[indM15 == 1, ]
indSc15 <-
sample(2, nrow(dataSc15), replace = TRUE, prob = c(.8, .2))
trainScU15 <- dataSc15[indSc15 == 1, ]
# Split 2015 into training set and test set
set.seed(100)
indSc15 <-
sample(2, nrow(dataSc15), replace = TRUE, prob = c(.8, .2))
trainScU15 <- dataSc15[indSc15 == 1, ]
testSc15 <- dataSc15[indSc15 == 2, ]
set.seed(100)
indM15 <- sample(2, nrow(dataM15), replace = TRUE, prob = c(.8, .2))
trainMU15 <- dataM15[indM15 == 1, ]
testM15 <- dataM15[indM15 == 2, ]
# Oversample traning set (but not test set!)
trainSc15 <- random_oversample(form = Sc ~ ., dat = trainScU15)
trainM15 <- random_oversample(form = M ~ ., dat = trainMU15)
# Picture a tree
dev.off()
model <- rpart(M ~ ., data = trainM15)
View(trainM15)
rpart.plot(model, type = 0)
# Picture a tree
dev.off()
model <- rpart(M ~ ., data = trainM15)
rpart.plot(model, type = 0)
rpart.plot(model, type = 3)
rpart.plot(model, type = 0)
model <- rpart(M ~ ., data = trainM15)
rpart.plot(model, type = 0)
rpart.plot(model, type = 3)
0
0
0
# Picture a tree
dev.off()
model <- rpart(M ~ ., data = trainM15)
rpart.plot(model, type = 0)
# Model RF
# Mathematics
set.seed(100)
rfM <- randomForest(M ~ ., data = trainM15, importance = T)
# Science
set.seed(100)
rfSc <- randomForest(Sc ~ ., data = trainSc15, importance = T)
# Importance plots
par(mfrow = c(1, 2))
varImpPlot(rfM,
type = 1,
scale = F,
main = "Mathematics")
varImpPlot(rfSc,
type = 1,
scale = F,
main = "Science")
varImpPlot(rfM,
#type = 1,
scale = F,
main = "Mathematics")
varImpPlot(rfSc,
type = 1,
scale = F,
main = "Science")
# Importance plots
par(mfrow = c(1, 2))
varImpPlot(rfM,
#type = 1,
scale = F,
main = "Mathematics")
varImpPlot(rfSc,
type = 1,
scale = F,
main = "Science")
varImpPlot(rfM,
#type = 1,
scale = F,
main = "Mathematics")
varImpPlot(rfM,
type = 1,
scale = F,
main = "Mathematics")
varImpPlot(rfSc,
type = 1,
scale = F,
main = "Science")
# Evaluate the models
# Mathematics 2015, test set
fittedM15 <- predict(rfM, newdata = testM15)
# Mathematics 2018, the whole dataset
fittedM18 <- predict(rfM, newdata = dataM18)
# Science 2015, test set
fittedSc15 <- predict(rfSc, newdata = testSc15)
# Science 2018, the whole dataset
fittedSc18 <- predict(rfSc, newdata = dataSc18)
# Multiclass AUCs
aucM15 <-
multiclass.roc(as.numeric(fittedM15), as.numeric(testM15$M), percent = T)
View(trainSc15)
View(trainMU15)
aucM18 <-
multiclass.roc(as.numeric(fittedM18), as.numeric(dataM18$M), percent = T)
aucSc15 <-
multiclass.roc(as.numeric(fittedSc15), as.numeric(testSc15$Sc), percent = T)
aucSc18 <-
multiclass.roc(as.numeric(fittedSc18), as.numeric(dataSc18$Sc), percent = T)
View(aucM15)
View(aucM18)
View(aucM15)
View(aucM18)
View(aucSc15)
View(C)
View(aucSc18)
View(aucSc15)
View(aucM18)
View(aucM15)
View(aucSc18)
View(aucM18)
View(aucM15)
View(aucSc15)
View(aucM15)
View(aucM18)
View(aucSc18)
# AUC plots, four in one
par(pty = "s")
layout(
mat = matrix(c(1, 2, 3, 4, 5, 5, 5, 5), nrow = 2, byrow = TRUE),
heights = c(0.4, 0.2)
)
# Mathematics 2015
plot(
smooth(roc(
as.numeric(fittedM15),
as.numeric(testM15$M),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedM15),
as.numeric(testM15$M),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedM15),
as.numeric(testM15$M),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Mathematics 2015", adj = 0.7, line = -6)
# Science 2015
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Science 2015", adj = 0.7, line = -6)
# Mathematics 2018
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Mathematics 2018", adj = 0.7, line = -6)
# Science 2018
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Science 2018", adj = 0.7, line = -6)
# Legend
par(mar = c(0, 0, 0, 0))
plot(
1,
type = "n",
axes = FALSE,
xlab = "",
ylab = ""
)
legend(
x = "top",
legend = c("1-3", "2-3", "1-2"),
lty = 1:3,
title = "Class Comparisons",
xpd = NA,
horiz = T, # was T
inset = 0,
pch = NA,
seg.len = 3
)
print(aucM15$auc)
# Plot one predictor pdps
# Mathematics model
dev.off()
par(mfrow = c(2, 5))
# Class 1
for (j in 1:5) {
plot(
partial(
rfM,
pred.var = names(trainM15[j]),
train = trainM15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "1"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainM15[j]))),
ylab = "MTH-1"
)
}
# Class 3
for (j in 1:5) {
plot(
partial(
rfM,
pred.var = names(trainM15[j]),
train = trainM15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "3"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainM15[j]))),
ylab = "MTH-3"
)
}
# Science model
# Class 1
for (j in 1:5) {
plot(
partial(
rfSc,
pred.var = names(trainSc15[j]),
train = trainSc15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "1"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainSc15[j]))),
ylab = "SCI-1"
)
}
# Class 3
for (j in 1:5) {
plot(
partial(
rfSc,
pred.var = names(trainSc15[j]),
train = trainSc15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "3"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainSc15[j]))),
ylab = "SCI-3"
)
}
# 3D pdps, example for ESCS and autonomy in science model
pdAE1 <- partial(
rfSc,
train = trainSc15,
pred.var = c("AUTO", "ESCS"),
grid.resolution = 40,
which.class = "1"
)
plotPartial(
pdAE1,
levelplot = FALSE,
zlab      = "SCI-1",
drape     = TRUE,
colorkey  = FALSE,
screen    = list(z = -40, x = -40)
)
