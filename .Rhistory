as.numeric(testM15$M),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedM15),
as.numeric(testM15$M),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Mathematics 2015", adj = 0.7, line = -6)
# Science 2015
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedSc15),
as.numeric(testSc15$Sc),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Science 2015", adj = 0.7, line = -6)
# Mathematics 2018
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedM18),
as.numeric(dataM18$M),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Mathematics 2018", adj = 0.7, line = -6)
# Science 2018
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("1", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 1,
xlab = "False Positive Rate",
ylab = "True Positive Rate"
)
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("2", "3")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 2,
add = T
)
plot(
smooth(roc(
as.numeric(fittedSc18),
as.numeric(dataSc18$Sc),
levels = c("1", "2")
)),
xlim = c(1, 0),
legacy.axes = T,
xaxs = "i",
yaxs = "i",
lty = 3,
add = T
)
title(xlab = "Science 2018", adj = 0.7, line = -6)
# Legend
par(mar = c(0, 0, 0, 0))
plot(
1,
type = "n",
axes = FALSE,
xlab = "",
ylab = ""
)
legend(
x = "top",
legend = c("1-3", "2-3", "1-2"),
lty = 1:3,
title = "Class Comparisons",
xpd = NA,
horiz = T, # was T
inset = 0,
pch = NA,
seg.len = 3
)
# Plot one predictor pdps
# Mathematics model
dev.off()
par(mfrow = c(2, 5))
# Class 1
for (j in 1:5) {
plot(
partial(
rfM,
pred.var = names(trainM15[j]),
train = trainM15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "1"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainM15[j]))),
ylab = "MTH-1"
)
}
# Class 3
for (j in 1:5) {
plot(
partial(
rfM,
pred.var = names(trainM15[j]),
train = trainM15,
type = "classification",
prob = T,
chull = TRUE,
which.class = "3"
),
type = "l",
ylim = c(0, 0.5),
yaxs = "i",
xlab = print(paste(names(trainM15[j]))),
ylab = "MTH-3"
)
}
# 3D pdps, example for ESCS and autonomy in science model
pdAE1 <- partial(
rfSc,
train = trainSc15,
pred.var = c("AUTO", "ESCS"),
grid.resolution = 40,
which.class = "1"
)
plotPartial(
pdAE1,
levelplot = FALSE,
zlab      = "SCI-1",
drape     = TRUE,
colorkey  = FALSE,
screen    = list(z = -40, x = -40)
)
# Libraries
# Data manipulation and visualization
library(tidyverse)
library(haven)
library(lattice)
library(car)
library(sjPlot)
# Missing data
library(VIM)
library(missForest)
# Decision tree
library(rpart)
library(rpart.plot)
# Classification task
# library(UBL)  # UBL package is no longer supported
library(randomForest)
library(pROC)
library(pdp)
# Multilevel models
library(lme4)
library(lmerTest)
library(performance)
# Select variables: demographics, ICT, maths, science, and weights
# dataA is the initial dataset with our variables of interest selected from German data
dataA15 <- dplyr::select(
DEUdata15,
c(
"CNTSCHID",
"ST004D01T",
"ESCS",
"INTICT",
"COMPICT",
"AUTICT",
"SOIAICT",
intersect(starts_with("PV"), ends_with("MATH")),
intersect(starts_with("PV"), ends_with("SCIE")),
starts_with("W_FS")
)
)
# Statistical part, HLM
# Run the script to the end first for the 2015 data
dataI <- dataI15
dataB <- dataB15
# Prepare the final dataset
# ICT variables and ESCS: center and standardise by two sds
DataForStd <-
as.data.frame(scale(dataI, center = TRUE, scale = FALSE))
twosd <- function(i) {
0.5 * i / sd(i)
}
StanData <- as.data.frame(apply(DataForStd, 2, twosd))
View(StanData)
View(dataI15)
#Gender: as factor & recode to 0 and 1
GEND <- as.data.frame(as.factor(car::recode(dataB$GEND, "'2' = 1; '1' = 0")))
View(GEND)
names(GEND) <- "GEND"
View(GEND)
#Gender: as factor & recode to 0 and 1
GEND <- as.data.frame(as.factor(car::recode(dataB$GEND, "'2' = 1; '1' = 0")))
names(GEND) <- "GEND"
View(dataB)
# PVs and school ID: leave as they were
LeftData <-
dplyr::select(dataB, c("SCHL", starts_with("PV")))
# Weights: correct for the sample size
WeightDataOld <- dplyr::select(dataB, starts_with("W_FS"))
View(WeightDataOld)
correctweight <- function(i) {
nrow(WeightDataOld) * i / sum(i)
}
WeightDataNew <-
as.data.frame(apply(WeightDataOld, 2, correctweight))
# Merge
mydata <- cbind(StanData, GEND, LeftData, WeightDataNew)
Nstud <- table(mydata$SCHL)
sum(Nstud < 10) # number of groups with less than 10 students
# Multilevel models
# Select math and science PVs and weights
PVsMath     <- dplyr::select(mydata, contains("MATH"))
PVsScie     <- dplyr::select(mydata, contains("SCIE"))
WeightsData <- dplyr::select(mydata, contains("W_FS"))
# Null (unconditional) models
# Variance decomposition and ICC
nullmodvar <- function(i) {
summary(lmer(i ~ 1 + (1 | SCHL), mydata, weights = W_FSTUWT))$varcor
}
NullMathVar <-
dplyr::select(as.data.frame(apply(PVsMath, 2, nullmodvar)), contains("vcov"))
NullMathInCptVar <- rowMeans(NullMathVar[1, ])
NullMathResidVar <- rowMeans(NullMathVar[2, ])
ICCmath <- NullMathInCptVar / (NullMathInCptVar + NullMathResidVar)
NullScieVar <-
dplyr::select(as.data.frame(apply(PVsScie, 2, nullmodvar)), contains("vcov"))
NullScieInCptVar <- rowMeans(NullScieVar[1, ])
NullScieResidVar <- rowMeans(NullScieVar[2, ])
ICCscie          <-
NullScieInCptVar / (NullScieInCptVar + NullScieResidVar)
ICCscie
ICCmath
# Fixed effects in null models (intercept estimate)
nullmodFixed <- function(i) {
lme4::fixef(lmer(i ~ 1 + (1 | SCHL), mydata, weights = W_FSTUWT))
}
NullMathFixed <- as.data.frame(apply(PVsMath, 2, nullmodFixed))
NullScieFixed <- as.data.frame(apply(PVsScie, 2, nullmodFixed))
NullMathInCpt <- colMeans(NullMathFixed) # intercept maths
NullScieInCpt <- colMeans(NullScieFixed) # intercept science
# Significance of the estimate
nullmodsign <- function(i) {
summary(lmer(i ~ 1 + (1 | SCHL), mydata, weights = W_FSTUWT))$coef
}
NullMathSign <- as.data.frame(apply(PVsMath, 2, nullmodsign))[5, ]
NullScieSign <- as.data.frame(apply(PVsScie, 2, nullmodsign))[5, ]
# Standard errors for null models
# Imputation variance
nullimpvarM <- function(i) {
(i - NullMathInCpt) ^ 2
}
NullImpVarM <- 1 / 9 * sum(apply(NullMathFixed, 1, nullimpvarM))
nullimpvarS <- function(i) {
(i - NullScieInCpt) ^ 2
}
NullImpVarS <- 1 / 9 * sum(apply(NullScieFixed, 1, nullimpvarS))
# Sampling variance
nullsamvar <- function(i) {
apply(WeightsData, 2, function(j) {
lme4::fixef(lmer(i ~ 1 + (1 | SCHL), mydata, weights = j))
})
}
NullSamM <- as.data.frame(apply(PVsMath, 2, nullsamvar))
NullSamS <- as.data.frame(apply(PVsScie, 2, nullsamvar))
NullSamVarM <-
1 / 10 * sum(1 / 20 * colSums((NullSamM - as.list(NullSamM[1, ])) ^ 2))
NullSamVarS <-
1 / 10 * sum(1 / 20 * colSums((NullSamS - as.list(NullSamS[1, ])) ^ 2))
# Standard errors
nullSEmath <- sqrt(NullSamVarM + 1.1 * NullImpVarM) # SE maths
nullSEscie <- sqrt(NullSamVarS + 1.1 * NullImpVarS) # SE science
# Random intercept models
finalmodFixed <- function(i) {
lme4::fixef(lmer(i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = W_FSTUWT))
}
# Fixed effects estimates
FinalMathFixed <- as.data.frame(apply(PVsMath, 2, finalmodFixed))
FinalScieFixed <- as.data.frame(apply(PVsScie, 2, finalmodFixed))
for (i in 1:7) {
print(paste(mean(as.numeric(FinalMathFixed[i, ]))))
}
for (i in 1:7) {
print(paste(mean(as.numeric(FinalScieFixed[i, ]))))
}
# Significance of estimates
finalmodsign <- function(i) {
car::Anova(lmer(i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = W_FSTUWT),
type = "III")$`Pr(>Chisq)`
}
FinalMathSign <- as.data.frame(apply(PVsMath, 2, finalmodsign))
FinalScieSign <- as.data.frame(apply(PVsScie, 2, finalmodsign))
# Variance decomposition for full models
finalmodVar <- function(i) {
summary(lmer(i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = W_FSTUWT))$varcor
}
FinMathVar <-
dplyr::select(as.data.frame(apply(PVsMath, 2, finalmodVar)), contains("vcov"))
FinIntVarM <- rowMeans(FinMathVar[1, ])
FinResVarM <- rowMeans(FinMathVar[2, ])
FinScieVar <-
dplyr::select(as.data.frame(apply(PVsScie, 2, finalmodVar)), contains("vcov"))
FinIntVarS <- rowMeans(FinScieVar[1, ])
FinResVarS <- rowMeans(FinScieVar[2, ])
# Nakagawa R2
nakagawa <- function(i) {
r2_nakagawa(lmer(i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = W_FSTUWT))
}
nakmathAll  <- unname(unlist(apply(PVsMath, 2, nakagawa)))
nakscieAll  <- unname(unlist(apply(PVsScie, 2, nakagawa)))
mean(nakmathAll[seq(1, 19, 2)])#conditional
mean(nakmathAll[seq(2, 20, 2)])#marginal
mean(nakscieAll[seq(1, 19, 2)])#conditional
mean(nakscieAll[seq(2, 20, 2)])#marginal
# Standard errors for full models
# Imputation variance
ImpVarM <- matrix(, nrow = 7, ncol = 1)
for (i in 1:7) {
ImpVarM[i, 1] <-
1 / 9 * sum((FinalMathFixed[i, ] - rowMeans(FinalMathFixed[i, ])) ^ 2)
}
ImpVarS <- matrix(, nrow = 7, ncol = 1)
for (i in 1:7) {
ImpVarS[i, 1] <-
1 / 9 * sum((FinalScieFixed[i, ] - rowMeans(FinalScieFixed[i, ])) ^ 2)
}
# Sampling variance
finsamvar <- function(i) {
apply(WeightsData, 2, function(j) {
lme4::fixef(lmer(
i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = j
))
})
}
FinSamM <- as.data.frame(apply(PVsMath, 2, finsamvar))
SamVarM <- matrix(, nrow = 7, ncol = 1)
for (i in 1:7) {
SamVarM[i, 1] <-
1 / 10 *
sum(1 / 20 *
colSums((FinSamM[seq(i, nrow(FinSamM), 7),]
- as.list(FinSamM[seq(i, nrow(FinSamM), 7),][1,])) ^ 2))
}
FinSamS <- as.data.frame(apply(PVsScie, 2, finsamvar))
SamVarS <- matrix(, nrow = 7, ncol = 1)
for (i in 1:7) {
SamVarS[i, 1] <-
1 / 10 *
sum(1 / 20 *
colSums((FinSamS[seq(i, nrow(FinSamS), 7), ]
- as.list(FinSamS[seq(i, nrow(FinSamS), 7), ][1, ])) ^ 2))
}
# Standard errors
SEmathFin <- sqrt(SamVarM + 1.1 * ImpVarM)
SEscieFin <- sqrt(SamVarS + 1.1 * ImpVarS)
# Check assumptions for all full models
finalmod <- function(i) {
lmer(i ~ GEND + ESCS + COMP + INTE + SOCI + AUTO
+ (1 | SCHL),
mydata,
weights = W_FSTUWT)
}
fmM <- apply(PVsMath, 2, finalmod)
fmS <- apply(PVsScie, 2, finalmod)
# Diagnostic plots with sjPlot: residuals
diagplotM <- function(i) {
sjPlot::plot_model(fmM[[i]], type = "diag")
}
lapply(c(1:10), diagplotM)
# Diagnostic plots with sjPlot: residuals
diagplotM <- function(i) {
sjPlot::plot_model(fmM[[i]], type = "diag")
}
lapply(c(1:10), diagplotM)
install.packages("glmmTMB")
lapply(c(1:10), diagplotM)
diagplotS <- function(i) {
sjPlot::plot_model(fmS[[i]], type = "diag")
}
lapply(c(1:10), diagplotS)
# Check for no multicollinearity with VIF
for (i in 1:10) {
print(paste(car::vif(fmM[[i]])))
}
for (i in 1:10) {
print(paste(car::vif(fmS[[i]])))
}
# Visualize models
# Plot estimates, all PVs
dev.off()
sjPlot::plot_models(fmM, show.legend = F, title = "Mathematics 2015") # change the title for 2018
sjPlot::plot_models(fmS, show.legend = F, title = "Science 2015") # change the title for 2018
# Statistical part, HLM
# Run the script to the end first for the 2015 data
dataI <- dataI15
dataB <- dataB15
View(dataI)
View(dataB)
View(DEUdata15)
NullMathVar <-
dplyr::select(as.data.frame(apply(PVsMath, 2, nullmodvar)), contains("vcov"))
NullMathVar
